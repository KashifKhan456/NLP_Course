{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8I5io0X2AXHscXXFFG/M4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KashifKhan456/Regexes_In_NLP/blob/main/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**\n"
      ],
      "metadata": {
        "id": "TXTtcnn8kbIq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ3zhtYMgBWc",
        "outputId": "84223d33-07d5-4297-e35c-d4e8743496d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convert all text to lowercase to ensure uniformity and prevent duplication due to case differences !!!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "text = 'Convert all text to lowercase to Ensure uniformity and Prevent duplication due to case Differences !!!'\n",
        "lower_text = text.lower()\n",
        "print(lower_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "caY1F7o0lIGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flKeLQFJZf_h",
        "outputId": "39178c8f-9986-448a-c02e-d197edb2ff91"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Convert', 'all', 'text', 'to', 'lowercase', 'to', 'Ensure', 'uniformity', 'and', 'Prevent', 'duplication', 'due', 'to', 'case', 'Differences', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removal of stopwords**"
      ],
      "metadata": {
        "id": "AvpQ4AsLlNlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove common stop words (e.g., \"the,\" \"is,\" \"in\") that do not carry significant meaning.\n",
        "from nltk.corpus import stopwords\n",
        "remove_stopwords = set(stopwords.words('english'))\n",
        "filtered_data = [word for word in tokens if word.lower() not in remove_stopwords]\n",
        "print(filtered_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7IuFZo4bPmI",
        "outputId": "617de0c6-3d5c-45f1-c657-78ba799874ac"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Convert', 'text', 'lowercase', 'Ensure', 'uniformity', 'Prevent', 'duplication', 'due', 'case', 'Differences', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Punctuation and Special Characters**"
      ],
      "metadata": {
        "id": "95_GfhrTptra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminate punctuation marks and special characters that are not essential for analysis.\n",
        "import re\n",
        "pattern = '[^\\w\\s]*'\n",
        "remove_puncuation = re.sub(pattern, '', str(filtered_data))\n",
        "print(remove_puncuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b--PQ4KPl9T_",
        "outputId": "8fed3a13-da9c-4dd8-e969-80944903de9f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert text lowercase Ensure uniformity Prevent duplication due case Differences   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming and Lemmatization**"
      ],
      "metadata": {
        "id": "rOp-ieAVwydB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer # import PorterStemmer for stemming process\n",
        "from nltk.stem import WordNetLemmatizer # import WordNetLemmatizer for lemmatization process\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "# creating instance of stemming\n",
        "stemmer = PorterStemmer()\n",
        "# Creating instance of lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# text\n",
        "word = 'beauty'\n",
        "stemmer_result = stemmer.stem(word)\n",
        "lemmatize_result = lemmatizer.lemmatize(word)\n",
        "\n",
        "print(stemmer_result, lemmatize_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WprtN6P9sIy7",
        "outputId": "c19382dd-ffb6-492f-ddb9-1cfe4a03261c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beauti beauty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Encoding**"
      ],
      "metadata": {
        "id": "d2LdtK_ToWF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_label = label_encoder.fit_transform(filtered_data)\n",
        "print(encoded_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFi70CJ07598",
        "outputId": "4f157c9a-7ee2-4f4c-ce72-bf00956a9d21"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  9  8  3 10  4  7  6  5  2  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF Vectorizer**"
      ],
      "metadata": {
        "id": "Qubxl7vRoPRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "text1 = 'This is simple text to test the tfidfvectorizer, is this simple ?'\n",
        "tfidf_matrix = vectorizer.fit_transform(filtered_data)\n",
        "print(tfidf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNZHvVOR_rv0",
        "outputId": "c2e6ff3a-7c11-47b6-fe68-e19492955f97"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 1)\t1.0\n",
            "  (1, 8)\t1.0\n",
            "  (2, 6)\t1.0\n",
            "  (3, 5)\t1.0\n",
            "  (4, 9)\t1.0\n",
            "  (5, 7)\t1.0\n",
            "  (6, 4)\t1.0\n",
            "  (7, 3)\t1.0\n",
            "  (8, 0)\t1.0\n",
            "  (9, 2)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Count Vectorizer**"
      ],
      "metadata": {
        "id": "PS6ajDcin8FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "# Instance of the CountVectorizer\n",
        "text = [\"this is first document\", \"this is second document\", \"third document\",\"last document is\"]\n",
        "vec_count = CountVectorizer()\n",
        "\n",
        "result_data = vec_count.fit_transform(text)\n",
        "df = pd.DataFrame(result_data.toarray(), columns=vec_count.get_feature_names_out())\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)\n",
        "# print(result_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOREPurJBqx_",
        "outputId": "11e93268-94ee-4e86-d108-cb873e506943"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   document  first  is  last  second  third  this\n",
            "0         1      1   1     0       0      0     1\n",
            "1         1      0   1     0       1      0     1\n",
            "2         1      0   0     0       0      1     0\n",
            "3         1      0   1     1       0      0     0\n"
          ]
        }
      ]
    }
  ]
}